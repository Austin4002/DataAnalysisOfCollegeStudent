[INFO][2021-03-18 19:01|:45][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-03-18 19:01|:46][org.apache.spark.SparkContext]Submitted application: hive
[INFO][2021-03-18 19:01|:46][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-03-18 19:01|:46][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-03-18 19:01|:46][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-03-18 19:01|:46][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-03-18 19:01|:46][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-03-18 19:01|:47][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 6892.
[INFO][2021-03-18 19:01|:47][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-03-18 19:01|:47][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-03-18 19:01|:47][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-03-18 19:01|:47][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-03-18 19:01|:47][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-2373a41f-75f8-4727-8d36-c40ac2a0d854
[INFO][2021-03-18 19:01|:47][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-03-18 19:01|:47][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-03-18 19:01|:47][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-03-18 19:01|:47][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-03-18 19:01|:47][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Connecting to master spark://spark1:7077...
[INFO][2021-03-18 19:01|:47][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to spark1/192.168.195.11:7077 after 33 ms (0 ms spent in bootstraps)
[INFO][2021-03-18 19:01|:48][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Connected to Spark cluster with app ID app-20210318190148-0012
[INFO][2021-03-18 19:01|:48][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor added: app-20210318190148-0012/0 on worker-20210318171147-192.168.195.11-37541 (192.168.195.11:37541) with 2 cores
[INFO][2021-03-18 19:01|:48][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Granted executor ID app-20210318190148-0012/0 on hostPort 192.168.195.11:37541 with 2 cores, 1024.0 MB RAM
[INFO][2021-03-18 19:01|:48][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor updated: app-20210318190148-0012/0 is now RUNNING
[INFO][2021-03-18 19:01|:48][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 6916.
[INFO][2021-03-18 19:01|:48][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:6916
[INFO][2021-03-18 19:01|:48][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-03-18 19:01|:48][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 6916, None)
[INFO][2021-03-18 19:01|:48][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:6916 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 6916, None)
[INFO][2021-03-18 19:01|:48][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 6916, None)
[INFO][2021-03-18 19:01|:48][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 6916, None)
[INFO][2021-03-18 19:01|:48][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO][2021-03-18 19:01|:48][org.apache.spark.sql.internal.SharedState]loading hive config file: file:/D:/ideaProject/DataAnalysisOfCollegeStudent/target/classes/hive-site.xml
[INFO][2021-03-18 19:01|:48][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse').
[INFO][2021-03-18 19:01|:48][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse'.
[INFO][2021-03-18 19:01|:50][org.apache.spark.sql.hive.HiveUtils]Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[WARN][2021-03-18 19:01|:51][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:01|:51][hive.metastore]Trying to connect to metastore with URI thrift://spark1:9083
[INFO][2021-03-18 19:01|:51][hive.metastore]Connected to metastore.
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/822b17fb-611f-418f-a111-98eb954a558a_resources
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/822b17fb-611f-418f-a111-98eb954a558a
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/822b17fb-611f-418f-a111-98eb954a558a
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/822b17fb-611f-418f-a111-98eb954a558a/_tmp_space.db
[INFO][2021-03-18 19:01|:51][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[WARN][2021-03-18 19:01|:51][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/69d3d0ba-ff11-47e1-a5dc-ffcedeb08df4_resources
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/69d3d0ba-ff11-47e1-a5dc-ffcedeb08df4
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/69d3d0ba-ff11-47e1-a5dc-ffcedeb08df4
[INFO][2021-03-18 19:01|:51][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/69d3d0ba-ff11-47e1-a5dc-ffcedeb08df4/_tmp_space.db
[INFO][2021-03-18 19:01|:51][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[INFO][2021-03-18 19:01|:51][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2021-03-18 19:01|:51][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: show databases
[INFO][2021-03-18 19:01|:52][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.195.11:34538) with ID 0
[INFO][2021-03-18 19:01|:53][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.11:45576 with 366.3 MB RAM, BlockManagerId(0, 192.168.195.11, 45576, None)
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 205.606553 ms
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 8.531824 ms
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: select * from test.city_info
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: bigint
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:01|:54][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: array<string>
[INFO][2021-03-18 19:01|:54][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2021-03-18 19:01|:54][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.195.1:4040
[INFO][2021-03-18 19:01|:54][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Shutting down all executors
[INFO][2021-03-18 19:01|:54][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Asking each executor to shut down
[INFO][2021-03-18 19:01|:54][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-03-18 19:01|:54][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2021-03-18 19:01|:54][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2021-03-18 19:01|:54][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2021-03-18 19:01|:54][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2021-03-18 19:01|:54][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2021-03-18 19:01|:54][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2021-03-18 19:01|:54][org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-a5debd7d-8820-4a84-ad1b-97cb7ef938d4
[INFO][2021-03-18 19:02|:44][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-03-18 19:02|:45][org.apache.spark.SparkContext]Submitted application: hive
[INFO][2021-03-18 19:02|:45][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-03-18 19:02|:45][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-03-18 19:02|:45][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-03-18 19:02|:45][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-03-18 19:02|:45][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-03-18 19:02|:46][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 7016.
[INFO][2021-03-18 19:02|:46][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-03-18 19:02|:46][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-03-18 19:02|:46][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-03-18 19:02|:46][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-03-18 19:02|:46][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-59ef9cf4-0690-4fed-844b-7c7884bc860a
[INFO][2021-03-18 19:02|:46][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-03-18 19:02|:46][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-03-18 19:02|:47][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-03-18 19:02|:47][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-03-18 19:02|:47][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Connecting to master spark://spark1:7077...
[INFO][2021-03-18 19:02|:47][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to spark1/192.168.195.11:7077 after 34 ms (0 ms spent in bootstraps)
[INFO][2021-03-18 19:02|:47][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Connected to Spark cluster with app ID app-20210318190247-0013
[INFO][2021-03-18 19:02|:47][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor added: app-20210318190247-0013/0 on worker-20210318171147-192.168.195.11-37541 (192.168.195.11:37541) with 2 cores
[INFO][2021-03-18 19:02|:47][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Granted executor ID app-20210318190247-0013/0 on hostPort 192.168.195.11:37541 with 2 cores, 1024.0 MB RAM
[INFO][2021-03-18 19:02|:47][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor updated: app-20210318190247-0013/0 is now RUNNING
[INFO][2021-03-18 19:02|:47][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7039.
[INFO][2021-03-18 19:02|:47][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:7039
[INFO][2021-03-18 19:02|:47][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-03-18 19:02|:47][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 7039, None)
[INFO][2021-03-18 19:02|:47][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:7039 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 7039, None)
[INFO][2021-03-18 19:02|:47][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 7039, None)
[INFO][2021-03-18 19:02|:47][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 7039, None)
[INFO][2021-03-18 19:02|:47][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO][2021-03-18 19:02|:47][org.apache.spark.sql.internal.SharedState]loading hive config file: file:/D:/ideaProject/DataAnalysisOfCollegeStudent/target/classes/hive-site.xml
[INFO][2021-03-18 19:02|:47][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse').
[INFO][2021-03-18 19:02|:47][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse'.
[INFO][2021-03-18 19:02|:48][org.apache.spark.sql.hive.HiveUtils]Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[WARN][2021-03-18 19:02|:49][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:02|:49][hive.metastore]Trying to connect to metastore with URI thrift://spark1:9083
[INFO][2021-03-18 19:02|:49][hive.metastore]Connected to metastore.
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/730283b9-4a62-4e9f-85f6-78a53bf2767b_resources
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/730283b9-4a62-4e9f-85f6-78a53bf2767b
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/730283b9-4a62-4e9f-85f6-78a53bf2767b
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/730283b9-4a62-4e9f-85f6-78a53bf2767b/_tmp_space.db
[INFO][2021-03-18 19:02|:50][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[WARN][2021-03-18 19:02|:50][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/3750095e-284a-4f98-97ef-1d472239ca3b_resources
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/3750095e-284a-4f98-97ef-1d472239ca3b
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/3750095e-284a-4f98-97ef-1d472239ca3b
[INFO][2021-03-18 19:02|:50][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/3750095e-284a-4f98-97ef-1d472239ca3b/_tmp_space.db
[INFO][2021-03-18 19:02|:50][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[INFO][2021-03-18 19:02|:50][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2021-03-18 19:02|:50][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: show databases
[INFO][2021-03-18 19:02|:51][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.195.11:40812) with ID 0
[INFO][2021-03-18 19:02|:52][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.11:36014 with 366.3 MB RAM, BlockManagerId(0, 192.168.195.11, 36014, None)
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 231.899796 ms
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 9.212604 ms
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: select * from test.city_info
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: bigint
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: array<string>
[INFO][2021-03-18 19:02|:53][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 17.959272 ms
[INFO][2021-03-18 19:02|:53][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2021-03-18 19:02|:53][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.195.1:4040
[INFO][2021-03-18 19:02|:53][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Shutting down all executors
[INFO][2021-03-18 19:02|:53][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Asking each executor to shut down
[INFO][2021-03-18 19:02|:53][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-03-18 19:02|:53][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2021-03-18 19:02|:53][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2021-03-18 19:02|:53][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2021-03-18 19:02|:53][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2021-03-18 19:02|:53][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2021-03-18 19:02|:53][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2021-03-18 19:02|:53][org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-6c917910-2de1-4c35-81a3-eabee573320b
[INFO][2021-03-18 19:04|:36][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-03-18 19:04|:37][org.apache.spark.SparkContext]Submitted application: hive
[INFO][2021-03-18 19:04|:37][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-03-18 19:04|:37][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-03-18 19:04|:37][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-03-18 19:04|:37][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-03-18 19:04|:37][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-03-18 19:04|:38][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 7248.
[INFO][2021-03-18 19:04|:38][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-03-18 19:04|:38][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-03-18 19:04|:38][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-03-18 19:04|:38][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-03-18 19:04|:38][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-16065fe2-030a-4f06-9c46-0f0741293fed
[INFO][2021-03-18 19:04|:38][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-03-18 19:04|:38][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-03-18 19:04|:38][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-03-18 19:04|:38][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-03-18 19:04|:38][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Connecting to master spark://spark1:7077...
[INFO][2021-03-18 19:04|:38][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to spark1/192.168.195.11:7077 after 34 ms (0 ms spent in bootstraps)
[INFO][2021-03-18 19:04|:39][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Connected to Spark cluster with app ID app-20210318190439-0014
[INFO][2021-03-18 19:04|:39][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor added: app-20210318190439-0014/0 on worker-20210318171147-192.168.195.11-37541 (192.168.195.11:37541) with 2 cores
[INFO][2021-03-18 19:04|:39][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Granted executor ID app-20210318190439-0014/0 on hostPort 192.168.195.11:37541 with 2 cores, 1024.0 MB RAM
[INFO][2021-03-18 19:04|:39][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor updated: app-20210318190439-0014/0 is now RUNNING
[INFO][2021-03-18 19:04|:39][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7272.
[INFO][2021-03-18 19:04|:39][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:7272
[INFO][2021-03-18 19:04|:39][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-03-18 19:04|:39][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 7272, None)
[INFO][2021-03-18 19:04|:39][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:7272 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 7272, None)
[INFO][2021-03-18 19:04|:39][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 7272, None)
[INFO][2021-03-18 19:04|:39][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 7272, None)
[INFO][2021-03-18 19:04|:39][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO][2021-03-18 19:04|:39][org.apache.spark.sql.internal.SharedState]loading hive config file: file:/D:/ideaProject/DataAnalysisOfCollegeStudent/target/classes/hive-site.xml
[INFO][2021-03-18 19:04|:39][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse').
[INFO][2021-03-18 19:04|:39][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse'.
[INFO][2021-03-18 19:04|:40][org.apache.spark.sql.hive.HiveUtils]Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[WARN][2021-03-18 19:04|:41][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:04|:41][hive.metastore]Trying to connect to metastore with URI thrift://spark1:9083
[INFO][2021-03-18 19:04|:41][hive.metastore]Connected to metastore.
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/6e9b6792-8665-4994-ac71-58e396f76e8f_resources
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/6e9b6792-8665-4994-ac71-58e396f76e8f
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/6e9b6792-8665-4994-ac71-58e396f76e8f
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/6e9b6792-8665-4994-ac71-58e396f76e8f/_tmp_space.db
[INFO][2021-03-18 19:04|:42][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[WARN][2021-03-18 19:04|:42][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/35aca9f4-269e-4a84-962c-e32077a98c88_resources
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/35aca9f4-269e-4a84-962c-e32077a98c88
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/35aca9f4-269e-4a84-962c-e32077a98c88
[INFO][2021-03-18 19:04|:42][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/35aca9f4-269e-4a84-962c-e32077a98c88/_tmp_space.db
[INFO][2021-03-18 19:04|:42][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[INFO][2021-03-18 19:04|:42][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2021-03-18 19:04|:42][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: show databases
[INFO][2021-03-18 19:04|:44][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.195.11:60182) with ID 0
[INFO][2021-03-18 19:04|:44][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.11:40342 with 366.3 MB RAM, BlockManagerId(0, 192.168.195.11, 40342, None)
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 222.643015 ms
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 9.324745 ms
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: select * from test.city_info
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: bigint
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: array<string>
[INFO][2021-03-18 19:04|:45][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 21.037692 ms
[INFO][2021-03-18 19:04|:46][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 289.7 KB, free 1991.7 MB)
[INFO][2021-03-18 19:04|:47][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.9 KB, free 1991.7 MB)
[INFO][2021-03-18 19:04|:47][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.1:7272 (size: 24.9 KB, free: 1992.0 MB)
[INFO][2021-03-18 19:04|:47][org.apache.spark.SparkContext]Created broadcast 0 from 
[INFO][2021-03-18 19:04|:47][org.apache.spark.ContextCleaner]Cleaned accumulator 0
[INFO][2021-03-18 19:04|:48][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-18 19:04|:48][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.DAGScheduler]Got job 0 (main at <unknown>:0) with 1 output partitions
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (main at <unknown>:0)
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[4] at main at <unknown>:0), which has no missing parents
[INFO][2021-03-18 19:04|:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 8.1 KB, free 1991.7 MB)
[INFO][2021-03-18 19:04|:48][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1991.7 MB)
[INFO][2021-03-18 19:04|:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.195.1:7272 (size: 4.4 KB, free: 1992.0 MB)
[INFO][2021-03-18 19:04|:48][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1015
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2021-03-18 19:04|:48][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, 192.168.195.11, executor 0, partition 0, ANY, 4893 bytes)
[INFO][2021-03-18 19:04|:48][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.195.11:40342 (size: 4.4 KB, free: 366.3 MB)
[INFO][2021-03-18 19:04|:49][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.11:40342 (size: 24.9 KB, free: 366.3 MB)
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 3038 ms on 192.168.195.11 (executor 0) (1/1)
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (main at <unknown>:0) finished in 3.051 s
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: main at <unknown>:0, took 3.191810 s
[INFO][2021-03-18 19:04|:51][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Got job 1 (main at <unknown>:0) with 1 output partitions
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (main at <unknown>:0)
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[4] at main at <unknown>:0), which has no missing parents
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 8.1 KB, free 1991.7 MB)
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1991.7 MB)
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.195.1:7272 (size: 4.4 KB, free: 1992.0 MB)
[INFO][2021-03-18 19:04|:51][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1015
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at main at <unknown>:0) (first 15 tasks are for partitions Vector(1))
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, 192.168.195.11, executor 0, partition 1, ANY, 4893 bytes)
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.195.11:40342 (size: 4.4 KB, free: 366.3 MB)
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 105 ms on 192.168.195.11 (executor 0) (1/1)
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (main at <unknown>:0) finished in 0.107 s
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: main at <unknown>:0, took 0.123396 s
[INFO][2021-03-18 19:04|:51][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2021-03-18 19:04|:51][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.195.1:4040
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Shutting down all executors
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Asking each executor to shut down
[INFO][2021-03-18 19:04|:51][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2021-03-18 19:04|:51][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2021-03-18 19:04|:51][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2021-03-18 19:04|:51][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2021-03-18 19:04|:51][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2021-03-18 19:04|:51][org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-7f86b30f-67e9-4954-9123-fc33f6c71099
[INFO][2021-03-18 19:11|:56][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-03-18 19:11|:58][org.apache.spark.SparkContext]Submitted application: hive
[INFO][2021-03-18 19:11|:58][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-03-18 19:11|:58][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-03-18 19:11|:58][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-03-18 19:11|:58][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-03-18 19:11|:58][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-03-18 19:11|:58][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 7928.
[INFO][2021-03-18 19:11|:58][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-03-18 19:11|:58][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-03-18 19:11|:58][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-03-18 19:11|:58][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-03-18 19:11|:58][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-b69a4e8c-d13b-4d21-8e1d-fb4bdb32528a
[INFO][2021-03-18 19:11|:58][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-03-18 19:11|:58][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-03-18 19:11|:59][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-03-18 19:11|:59][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-03-18 19:11|:59][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Connecting to master spark://spark1:7077...
[INFO][2021-03-18 19:11|:59][org.apache.spark.network.client.TransportClientFactory]Successfully created connection to spark1/192.168.195.11:7077 after 31 ms (0 ms spent in bootstraps)
[INFO][2021-03-18 19:11|:59][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Connected to Spark cluster with app ID app-20210318191200-0015
[INFO][2021-03-18 19:11|:59][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7950.
[INFO][2021-03-18 19:11|:59][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:7950
[INFO][2021-03-18 19:11|:59][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor added: app-20210318191200-0015/0 on worker-20210318171147-192.168.195.11-37541 (192.168.195.11:37541) with 2 cores
[INFO][2021-03-18 19:11|:59][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Granted executor ID app-20210318191200-0015/0 on hostPort 192.168.195.11:37541 with 2 cores, 1024.0 MB RAM
[INFO][2021-03-18 19:11|:59][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-03-18 19:11|:59][org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint]Executor updated: app-20210318191200-0015/0 is now RUNNING
[INFO][2021-03-18 19:11|:59][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 7950, None)
[INFO][2021-03-18 19:11|:59][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:7950 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 7950, None)
[INFO][2021-03-18 19:11|:59][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 7950, None)
[INFO][2021-03-18 19:11|:59][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 7950, None)
[INFO][2021-03-18 19:12|:00][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[INFO][2021-03-18 19:12|:00][org.apache.spark.sql.internal.SharedState]loading hive config file: file:/D:/ideaProject/DataAnalysisOfCollegeStudent/target/classes/hive-site.xml
[INFO][2021-03-18 19:12|:00][org.apache.spark.sql.internal.SharedState]Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse').
[INFO][2021-03-18 19:12|:00][org.apache.spark.sql.internal.SharedState]Warehouse path is 'file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse'.
[INFO][2021-03-18 19:12|:01][org.apache.spark.sql.hive.HiveUtils]Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[WARN][2021-03-18 19:12|:02][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:12|:02][hive.metastore]Trying to connect to metastore with URI thrift://spark1:9083
[INFO][2021-03-18 19:12|:02][hive.metastore]Connected to metastore.
[INFO][2021-03-18 19:12|:02][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/637d5c9d-96ed-4eba-9a42-920f0296bbc4_resources
[INFO][2021-03-18 19:12|:02][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/637d5c9d-96ed-4eba-9a42-920f0296bbc4
[INFO][2021-03-18 19:12|:02][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/637d5c9d-96ed-4eba-9a42-920f0296bbc4
[INFO][2021-03-18 19:12|:02][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/637d5c9d-96ed-4eba-9a42-920f0296bbc4/_tmp_space.db
[INFO][2021-03-18 19:12|:02][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[WARN][2021-03-18 19:12|:03][org.apache.hadoop.hive.conf.HiveConf]HiveConf of name hive.metastore.local does not exist
[INFO][2021-03-18 19:12|:03][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/36c93cae-932d-4e7e-b474-20bdf61311ed_resources
[INFO][2021-03-18 19:12|:03][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/36c93cae-932d-4e7e-b474-20bdf61311ed
[INFO][2021-03-18 19:12|:03][org.apache.hadoop.hive.ql.session.SessionState]Created local directory: C:/Users/ADMINI~1/AppData/Local/Temp/Administrator/36c93cae-932d-4e7e-b474-20bdf61311ed
[INFO][2021-03-18 19:12|:03][org.apache.hadoop.hive.ql.session.SessionState]Created HDFS directory: /tmp/hive/Administrator/36c93cae-932d-4e7e-b474-20bdf61311ed/_tmp_space.db
[INFO][2021-03-18 19:12|:03][org.apache.spark.sql.hive.client.HiveClientImpl]Warehouse location for Hive client (version 1.2.1) is file:/D:/ideaProject/DataAnalysisOfCollegeStudent/spark-warehouse
[INFO][2021-03-18 19:12|:03][org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef]Registered StateStoreCoordinator endpoint
[INFO][2021-03-18 19:12|:03][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: show databases
[INFO][2021-03-18 19:12|:04][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.195.11:59722) with ID 0
[INFO][2021-03-18 19:12|:04][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.11:46230 with 366.3 MB RAM, BlockManagerId(0, 192.168.195.11, 46230, None)
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 227.493423 ms
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 7.329984 ms
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.execution.SparkSqlParser]Parsing command: select * from test.city_info
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: bigint
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: string
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.parser.CatalystSqlParser]Parsing command: array<string>
[INFO][2021-03-18 19:12|:05][org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator]Code generated in 43.595147 ms
[INFO][2021-03-18 19:12|:06][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 289.7 KB, free 1991.7 MB)
[INFO][2021-03-18 19:12|:06][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.9 KB, free 1991.7 MB)
[INFO][2021-03-18 19:12|:06][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.1:7950 (size: 24.9 KB, free: 1992.0 MB)
[INFO][2021-03-18 19:12|:06][org.apache.spark.SparkContext]Created broadcast 0 from 
[INFO][2021-03-18 19:12|:07][org.apache.spark.ContextCleaner]Cleaned accumulator 0
[INFO][2021-03-18 19:12|:07][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-18 19:12|:07][org.apache.spark.SparkContext]Starting job: hivesqlconnect at JavaDemo.java:8
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.DAGScheduler]Got job 0 (hivesqlconnect at JavaDemo.java:8) with 1 output partitions
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (hivesqlconnect at JavaDemo.java:8)
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (MapPartitionsRDD[4] at hivesqlconnect at JavaDemo.java:8), which has no missing parents
[INFO][2021-03-18 19:12|:07][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 8.1 KB, free 1991.7 MB)
[INFO][2021-03-18 19:12|:07][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1991.7 MB)
[INFO][2021-03-18 19:12|:07][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.195.1:7950 (size: 4.4 KB, free: 1992.0 MB)
[INFO][2021-03-18 19:12|:07][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1015
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at hivesqlconnect at JavaDemo.java:8) (first 15 tasks are for partitions Vector(0))
[INFO][2021-03-18 19:12|:07][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO][2021-03-18 19:12|:08][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, 192.168.195.11, executor 0, partition 0, ANY, 4893 bytes)
[INFO][2021-03-18 19:12|:08][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.195.11:46230 (size: 4.4 KB, free: 366.3 MB)
[INFO][2021-03-18 19:12|:08][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.11:46230 (size: 24.9 KB, free: 366.3 MB)
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 2350 ms on 192.168.195.11 (executor 0) (1/1)
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (hivesqlconnect at JavaDemo.java:8) finished in 2.370 s
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: hivesqlconnect at JavaDemo.java:8, took 2.487927 s
[INFO][2021-03-18 19:12|:10][org.apache.spark.SparkContext]Starting job: hivesqlconnect at JavaDemo.java:8
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Got job 1 (hivesqlconnect at JavaDemo.java:8) with 1 output partitions
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (hivesqlconnect at JavaDemo.java:8)
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[4] at hivesqlconnect at JavaDemo.java:8), which has no missing parents
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 8.1 KB, free 1991.7 MB)
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.4 KB, free 1991.7 MB)
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.195.1:7950 (size: 4.4 KB, free: 1992.0 MB)
[INFO][2021-03-18 19:12|:10][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1015
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at hivesqlconnect at JavaDemo.java:8) (first 15 tasks are for partitions Vector(1))
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 1, 192.168.195.11, executor 0, partition 1, ANY, 4893 bytes)
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.195.11:46230 (size: 4.4 KB, free: 366.3 MB)
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 1) in 143 ms on 192.168.195.11 (executor 0) (1/1)
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (hivesqlconnect at JavaDemo.java:8) finished in 0.144 s
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: hivesqlconnect at JavaDemo.java:8, took 0.164465 s
[INFO][2021-03-18 19:12|:10][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2021-03-18 19:12|:10][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.195.1:4040
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend]Shutting down all executors
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint]Asking each executor to shut down
[INFO][2021-03-18 19:12|:10][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2021-03-18 19:12|:10][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2021-03-18 19:12|:10][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2021-03-18 19:12|:10][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2021-03-18 19:12|:10][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2021-03-18 19:12|:10][org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-83b9bcd3-f6cb-4546-8d21-4501edd9084e
[INFO][2021-03-19 16:55|:11][org.apache.hadoop.conf.Configuration.deprecation]session.id is deprecated. Instead, use dfs.metrics.session-id
[INFO][2021-03-19 16:55|:11][org.apache.hadoop.metrics.jvm.JvmMetrics]Initializing JVM Metrics with processName=JobTracker, sessionId=
[INFO][2021-03-19 16:55|:11][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:55|:11][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:55|:11][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:55|:11][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:55|:11][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:55|:11][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local612265102_0001
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapreduce.Job]Running job: job_local612265102_0001
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local612265102_0001_m_000000_0
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@10228ba3
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/test.csv:0+972
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:55|:12][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LineRecordReader]Found UTF-8 BOM and skipped it
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 705; bufvoid = 104857600
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214220(104856880); length = 177/6553600
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Task:attempt_local612265102_0001_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/test.csv:0+972
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Task 'attempt_local612265102_0001_m_000000_0' done.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Final Counters for attempt_local612265102_0001_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=1123
		FILE: Number of bytes written=316742
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=972
		HDFS: Number of bytes written=972
		HDFS: Number of read operations=11
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=45
		Map output records=45
		Map output bytes=705
		Map output materialized bytes=801
		Input split bytes=98
		Combine input records=45
		Combine output records=45
		Spilled Records=45
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=279969792
	File Input Format Counters 
		Bytes Read=972
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local612265102_0001_m_000000_0
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.Job]Job job_local612265102_0001 running in uber mode : false
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.Job] map 100% reduce 0%
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local612265102_0001_r_000000_0
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@65684ab1
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5d8e4816
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local612265102_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#1 about to shuffle output of map attempt_local612265102_0001_m_000000_0 decomp: 797 len: 801 to MEMORY
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 797 bytes from map-output for attempt_local612265102_0001_m_000000_0
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 797, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->797
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 791 bytes
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 797 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 801 bytes from disk
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 791 bytes
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Task:attempt_local612265102_0001_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Task attempt_local612265102_0001_r_000000_0 is allowed to commit now
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local612265102_0001_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step1/_temporary/0/task_local612265102_0001_r_000000
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Task 'attempt_local612265102_0001_r_000000_0' done.
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.Task]Final Counters for attempt_local612265102_0001_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=2757
		FILE: Number of bytes written=317543
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=972
		HDFS: Number of bytes written=1896
		HDFS: Number of read operations=14
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=45
		Reduce shuffle bytes=801
		Reduce input records=45
		Reduce output records=45
		Spilled Records=45
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=279969792
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=924
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local612265102_0001_r_000000_0
[INFO][2021-03-19 16:55|:13][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.Job]Job job_local612265102_0001 completed successfully
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=3880
		FILE: Number of bytes written=634285
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1944
		HDFS: Number of bytes written=2868
		HDFS: Number of read operations=25
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=45
		Map output records=45
		Map output bytes=705
		Map output materialized bytes=801
		Input split bytes=98
		Combine input records=45
		Combine output records=45
		Reduce input groups=45
		Reduce shuffle bytes=801
		Reduce input records=45
		Reduce output records=45
		Spilled Records=90
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=559939584
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=972
	File Output Format Counters 
		Bytes Written=924
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local645746968_0002
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.Job]Running job: job_local645746968_0002
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local645746968_0002_m_000000_0
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@59957342
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+924
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 960; bufvoid = 104857600
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214220(104856880); length = 177/6553600
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Task:attempt_local645746968_0002_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+924
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Task 'attempt_local645746968_0002_m_000000_0' done.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Final Counters for attempt_local645746968_0002_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=2916
		FILE: Number of bytes written=632812
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1896
		HDFS: Number of bytes written=1896
		HDFS: Number of read operations=24
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=45
		Map output records=45
		Map output bytes=960
		Map output materialized bytes=76
		Input split bytes=106
		Combine input records=45
		Combine output records=3
		Spilled Records=3
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=418381824
	File Input Format Counters 
		Bytes Read=924
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local645746968_0002_m_000000_0
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local645746968_0002_r_000000_0
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@64fb4a7d
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6a3cb892
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local645746968_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#2 about to shuffle output of map attempt_local645746968_0002_m_000000_0 decomp: 72 len: 76 to MEMORY
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 72 bytes from map-output for attempt_local645746968_0002_m_000000_0
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 72, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->72
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 54 bytes
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 72 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 76 bytes from disk
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 54 bytes
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Task:attempt_local645746968_0002_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Task attempt_local645746968_0002_r_000000_0 is allowed to commit now
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local645746968_0002_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step2/_temporary/0/task_local645746968_0002_r_000000
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Task 'attempt_local645746968_0002_r_000000_0' done.
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.Task]Final Counters for attempt_local645746968_0002_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=3100
		FILE: Number of bytes written=632888
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1896
		HDFS: Number of bytes written=1957
		HDFS: Number of read operations=27
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=12
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=76
		Reduce input records=3
		Reduce output records=3
		Spilled Records=3
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=418381824
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=61
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local645746968_0002_r_000000_0
[INFO][2021-03-19 16:55|:14][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.Job]Job job_local645746968_0002 running in uber mode : false
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.Job]Job job_local645746968_0002 completed successfully
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=6016
		FILE: Number of bytes written=1265700
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3792
		HDFS: Number of bytes written=3853
		HDFS: Number of read operations=51
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=22
	Map-Reduce Framework
		Map input records=45
		Map output records=45
		Map output bytes=960
		Map output materialized bytes=76
		Input split bytes=106
		Combine input records=45
		Combine output records=3
		Reduce input groups=3
		Reduce shuffle bytes=76
		Reduce input records=3
		Reduce output records=3
		Spilled Records=6
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=836763648
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=924
	File Output Format Counters 
		Bytes Written=61
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local1002434238_0003
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.Job]Running job: job_local1002434238_0003
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1002434238_0003_m_000000_0
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@712eb38b
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+924
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 714; bufvoid = 104857600
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214220(104856880); length = 177/6553600
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Task]Task:attempt_local1002434238_0003_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+924
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Task]Task 'attempt_local1002434238_0003_m_000000_0' done.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1002434238_0003_m_000000_0: Counters: 22
	File System Counters
		FILE: Number of bytes read=3259
		FILE: Number of bytes written=949025
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2820
		HDFS: Number of bytes written=1957
		HDFS: Number of read operations=36
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Map-Reduce Framework
		Map input records=45
		Map output records=45
		Map output bytes=714
		Map output materialized bytes=810
		Input split bytes=106
		Combine input records=0
		Spilled Records=45
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=523763712
	File Input Format Counters 
		Bytes Read=924
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1002434238_0003_m_000000_0
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1002434238_0003_r_000000_0
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@65fe835a
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b397195
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local1002434238_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#3 about to shuffle output of map attempt_local1002434238_0003_m_000000_0 decomp: 806 len: 810 to MEMORY
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 806 bytes from map-output for attempt_local1002434238_0003_m_000000_0
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 806, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->806
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 800 bytes
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 806 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 810 bytes from disk
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 800 bytes
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:15][org.apache.hadoop.mapred.Task]Task:attempt_local1002434238_0003_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.Task]Task attempt_local1002434238_0003_r_000000_0 is allowed to commit now
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local1002434238_0003_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step3_1/_temporary/0/task_local1002434238_0003_r_000000
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.Task]Task 'attempt_local1002434238_0003_r_000000_0' done.
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1002434238_0003_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=4911
		FILE: Number of bytes written=949835
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2820
		HDFS: Number of bytes written=2881
		HDFS: Number of read operations=39
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=810
		Reduce input records=45
		Reduce output records=45
		Spilled Records=45
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=523763712
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=924
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1002434238_0003_r_000000_0
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.Job]Job job_local1002434238_0003 running in uber mode : false
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.Job]Job job_local1002434238_0003 completed successfully
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=8170
		FILE: Number of bytes written=1898860
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=5640
		HDFS: Number of bytes written=4838
		HDFS: Number of read operations=75
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=34
	Map-Reduce Framework
		Map input records=45
		Map output records=45
		Map output bytes=714
		Map output materialized bytes=810
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=810
		Reduce input records=45
		Reduce output records=45
		Spilled Records=90
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1047527424
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=924
	File Output Format Counters 
		Bytes Written=924
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local377901791_0004
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.Job]Running job: job_local377901791_0004
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local377901791_0004_m_000000_0
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5ae9b784
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step2/part-00000:0+61
[INFO][2021-03-19 16:55|:16][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 64; bufvoid = 104857600
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214388(104857552); length = 9/6553600
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Task:attempt_local377901791_0004_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step2/part-00000:0+61
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Task 'attempt_local377901791_0004_m_000000_0' done.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Final Counters for attempt_local377901791_0004_m_000000_0: Counters: 22
	File System Counters
		FILE: Number of bytes read=5068
		FILE: Number of bytes written=1264204
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2881
		HDFS: Number of bytes written=2881
		HDFS: Number of read operations=49
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=22
	Map-Reduce Framework
		Map input records=3
		Map output records=3
		Map output bytes=64
		Map output materialized bytes=76
		Input split bytes=106
		Combine input records=0
		Spilled Records=3
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=632291328
	File Input Format Counters 
		Bytes Read=61
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local377901791_0004_m_000000_0
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local377901791_0004_r_000000_0
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@759ed468
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b5924ab
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local377901791_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#4 about to shuffle output of map attempt_local377901791_0004_m_000000_0 decomp: 72 len: 76 to MEMORY
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 72 bytes from map-output for attempt_local377901791_0004_m_000000_0
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 72, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->72
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 54 bytes
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 72 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 76 bytes from disk
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 54 bytes
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Task:attempt_local377901791_0004_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Task attempt_local377901791_0004_r_000000_0 is allowed to commit now
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local377901791_0004_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step3_2/_temporary/0/task_local377901791_0004_r_000000
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Task 'attempt_local377901791_0004_r_000000_0' done.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.Task]Final Counters for attempt_local377901791_0004_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=5252
		FILE: Number of bytes written=1264280
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=2881
		HDFS: Number of bytes written=2942
		HDFS: Number of read operations=52
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=24
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=76
		Reduce input records=3
		Reduce output records=3
		Spilled Records=3
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=61
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local377901791_0004_r_000000_0
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.Job]Job job_local377901791_0004 running in uber mode : false
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.Job]Job job_local377901791_0004 completed successfully
[INFO][2021-03-19 16:55|:17][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=10320
		FILE: Number of bytes written=2528484
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=5762
		HDFS: Number of bytes written=5823
		HDFS: Number of read operations=101
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=46
	Map-Reduce Framework
		Map input records=3
		Map output records=3
		Map output bytes=64
		Map output materialized bytes=76
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=76
		Reduce input records=3
		Reduce output records=3
		Spilled Records=6
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1264582656
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=61
	File Output Format Counters 
		Bytes Written=61
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.conf.Configuration.deprecation]mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 2
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:2
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local1095706259_0005
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.Job]Running job: job_local1095706259_0005
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1095706259_0005_m_000000_0
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b150025
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step3_1/part-00000:0+924
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1095706259_0005_m_000001_0
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@d4d13fe
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step3_2/part-00000:0+61
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.Task]Task:attempt_local1095706259_0005_m_000001_0 is done. And is in the process of committing
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step3_2/part-00000:0+61
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.Task]Task 'attempt_local1095706259_0005_m_000001_0' done.
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1095706259_0005_m_000001_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=5766
		FILE: Number of bytes written=1582120
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3866
		HDFS: Number of bytes written=2942
		HDFS: Number of read operations=65
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=28
	Map-Reduce Framework
		Map input records=3
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=108
		Combine input records=0
		Combine output records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=834666496
	File Input Format Counters 
		Bytes Read=61
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1095706259_0005_m_000001_0
[INFO][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[WARN][2021-03-19 16:55|:18][org.apache.hadoop.mapred.LocalJobRunner]job_local1095706259_0005
java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at mapred.Step4$Step4_PartialMultiplyMapper.map(Step4.java:62)
	at mapred.Step4$Step4_PartialMultiplyMapper.map(Step4.java:30)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO][2021-03-19 16:55|:19][org.apache.hadoop.mapreduce.Job]Job job_local1095706259_0005 running in uber mode : false
[INFO][2021-03-19 16:55|:19][org.apache.hadoop.mapreduce.Job] map 100% reduce 0%
[INFO][2021-03-19 16:55|:19][org.apache.hadoop.mapreduce.Job]Job job_local1095706259_0005 failed with state FAILED due to: NA
[INFO][2021-03-19 16:55|:19][org.apache.hadoop.mapreduce.Job]Counters: 23
	File System Counters
		FILE: Number of bytes read=5766
		FILE: Number of bytes written=1582120
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3866
		HDFS: Number of bytes written=2942
		HDFS: Number of read operations=65
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=28
	Map-Reduce Framework
		Map input records=3
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=108
		Combine input records=0
		Combine output records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=834666496
	File Input Format Counters 
		Bytes Read=61
[INFO][2021-03-19 16:59|:35][org.apache.hadoop.conf.Configuration.deprecation]session.id is deprecated. Instead, use dfs.metrics.session-id
[INFO][2021-03-19 16:59|:35][org.apache.hadoop.metrics.jvm.JvmMetrics]Initializing JVM Metrics with processName=JobTracker, sessionId=
[INFO][2021-03-19 16:59|:35][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:59|:35][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local742238513_0001
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.Job]Running job: job_local742238513_0001
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local742238513_0001_m_000000_0
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:36][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@43a0363b
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/date.csv:0+8282
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 6567; bufvoid = 104857600
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26212012(104848048); length = 2385/6553600
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Task:attempt_local742238513_0001_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/date.csv:0+8282
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Task 'attempt_local742238513_0001_m_000000_0' done.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Final Counters for attempt_local742238513_0001_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=8433
		FILE: Number of bytes written=320336
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=8282
		HDFS: Number of bytes written=8282
		HDFS: Number of read operations=11
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=597
		Map output records=597
		Map output bytes=6567
		Map output materialized bytes=4395
		Input split bytes=98
		Combine input records=597
		Combine output records=30
		Spilled Records=30
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=279445504
	File Input Format Counters 
		Bytes Read=8282
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local742238513_0001_m_000000_0
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local742238513_0001_r_000000_0
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7010ebd6
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3a44ff79
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local742238513_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#1 about to shuffle output of map attempt_local742238513_0001_m_000000_0 decomp: 4391 len: 4395 to MEMORY
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 4391 bytes from map-output for attempt_local742238513_0001_m_000000_0
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 4391, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4391
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 4385 bytes
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 4391 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 4395 bytes from disk
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 4385 bytes
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Task:attempt_local742238513_0001_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Task attempt_local742238513_0001_r_000000_0 is allowed to commit now
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local742238513_0001_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step1/_temporary/0/task_local742238513_0001_r_000000
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Task 'attempt_local742238513_0001_r_000000_0' done.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.Task]Final Counters for attempt_local742238513_0001_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=17255
		FILE: Number of bytes written=324731
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=8282
		HDFS: Number of bytes written=12542
		HDFS: Number of read operations=14
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=30
		Reduce shuffle bytes=4395
		Reduce input records=30
		Reduce output records=30
		Spilled Records=30
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=279445504
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=4260
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local742238513_0001_r_000000_0
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.Job]Job job_local742238513_0001 running in uber mode : false
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.Job]Job job_local742238513_0001 completed successfully
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=25688
		FILE: Number of bytes written=645067
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=16564
		HDFS: Number of bytes written=20824
		HDFS: Number of read operations=25
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=597
		Map output records=597
		Map output bytes=6567
		Map output materialized bytes=4395
		Input split bytes=98
		Combine input records=597
		Combine output records=30
		Reduce input groups=30
		Reduce shuffle bytes=4395
		Reduce input records=30
		Reduce output records=30
		Spilled Records=60
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=558891008
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=8282
	File Output Format Counters 
		Bytes Written=4260
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:59|:37][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local714630392_0002
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.Job]Running job: job_local714630392_0002
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local714630392_0002_m_000000_0
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@33c69e35
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+4260
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 106424; bufvoid = 104857600
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26161188(104644752); length = 53209/6553600
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Task:attempt_local714630392_0002_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+4260
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Task 'attempt_local714630392_0002_m_000000_0' done.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Final Counters for attempt_local714630392_0002_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=17414
		FILE: Number of bytes written=639940
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=12542
		HDFS: Number of bytes written=12542
		HDFS: Number of read operations=24
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=30
		Map output records=13303
		Map output bytes=106424
		Map output materialized bytes=16
		Input split bytes=106
		Combine input records=13303
		Combine output records=1
		Spilled Records=1
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=421527552
	File Input Format Counters 
		Bytes Read=4260
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local714630392_0002_m_000000_0
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local714630392_0002_r_000000_0
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6698d000
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@685f32a3
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local714630392_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#2 about to shuffle output of map attempt_local714630392_0002_m_000000_0 decomp: 12 len: 16 to MEMORY
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 12 bytes from map-output for attempt_local714630392_0002_m_000000_0
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 12, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->12
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 6 bytes
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 12 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 16 bytes from disk
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 6 bytes
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Task:attempt_local714630392_0002_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Task attempt_local714630392_0002_r_000000_0 is allowed to commit now
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local714630392_0002_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step2/_temporary/0/task_local714630392_0002_r_000000
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Task 'attempt_local714630392_0002_r_000000_0' done.
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.Task]Final Counters for attempt_local714630392_0002_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=17478
		FILE: Number of bytes written=639956
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=12542
		HDFS: Number of bytes written=12552
		HDFS: Number of read operations=27
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=12
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=16
		Reduce input records=1
		Reduce output records=1
		Spilled Records=1
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=421527552
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=10
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local714630392_0002_r_000000_0
[INFO][2021-03-19 16:59|:38][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.Job]Job job_local714630392_0002 running in uber mode : false
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.Job]Job job_local714630392_0002 completed successfully
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=34892
		FILE: Number of bytes written=1279896
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=25084
		HDFS: Number of bytes written=25094
		HDFS: Number of read operations=51
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=22
	Map-Reduce Framework
		Map input records=30
		Map output records=13303
		Map output bytes=106424
		Map output materialized bytes=16
		Input split bytes=106
		Combine input records=13303
		Combine output records=1
		Reduce input groups=1
		Reduce shuffle bytes=16
		Reduce input records=1
		Reduce output records=1
		Spilled Records=2
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=843055104
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=4260
	File Output Format Counters 
		Bytes Written=10
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local706391748_0003
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.Job]Running job: job_local706391748_0003
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local706391748_0003_m_000000_0
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@32fc56a0
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+4260
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 16:59|:39][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[WARN][2021-03-19 16:59|:39][org.apache.hadoop.mapred.LocalJobRunner]job_local706391748_0003
java.lang.Exception: java.lang.NumberFormatException: For input string: """
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NumberFormatException: For input string: """
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Integer.parseInt(Integer.java:569)
	at java.lang.Integer.parseInt(Integer.java:615)
	at mapred.Step3$Step31_UserVectorSplitterMapper.map(Step3.java:33)
	at mapred.Step3$Step31_UserVectorSplitterMapper.map(Step3.java:25)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO][2021-03-19 16:59|:40][org.apache.hadoop.mapreduce.Job]Job job_local706391748_0003 running in uber mode : false
[INFO][2021-03-19 16:59|:40][org.apache.hadoop.mapreduce.Job] map 0% reduce 0%
[INFO][2021-03-19 16:59|:40][org.apache.hadoop.mapreduce.Job]Job job_local706391748_0003 failed with state FAILED due to: NA
[INFO][2021-03-19 16:59|:40][org.apache.hadoop.mapreduce.Job]Counters: 0
[INFO][2021-03-19 17:17|:44][org.apache.hadoop.conf.Configuration.deprecation]session.id is deprecated. Instead, use dfs.metrics.session-id
[INFO][2021-03-19 17:17|:44][org.apache.hadoop.metrics.jvm.JvmMetrics]Initializing JVM Metrics with processName=JobTracker, sessionId=
[INFO][2021-03-19 17:17|:44][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local1574142201_0001
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.Job]Running job: job_local1574142201_0001
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1574142201_0001_m_000000_0
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@470ef9b7
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/small.csv:0+199
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 17:17|:45][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 220; bufvoid = 104857600
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214316(104857264); length = 81/6553600
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Task:attempt_local1574142201_0001_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/small.csv:0+199
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Task 'attempt_local1574142201_0001_m_000000_0' done.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1574142201_0001_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=350
		FILE: Number of bytes written=317689
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=199
		HDFS: Number of bytes written=199
		HDFS: Number of read operations=11
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=21
		Map output records=21
		Map output bytes=220
		Map output materialized bytes=172
		Input split bytes=99
		Combine input records=21
		Combine output records=5
		Spilled Records=5
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=324534272
	File Input Format Counters 
		Bytes Read=199
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1574142201_0001_m_000000_0
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1574142201_0001_r_000000_0
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.Job]Job job_local1574142201_0001 running in uber mode : false
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.Job] map 100% reduce 0%
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@39dbc8b0
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6a2f328c
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local1574142201_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#1 about to shuffle output of map attempt_local1574142201_0001_m_000000_0 decomp: 168 len: 172 to MEMORY
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 168 bytes from map-output for attempt_local1574142201_0001_m_000000_0
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 168, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->168
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 162 bytes
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 168 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 172 bytes from disk
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 162 bytes
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Task:attempt_local1574142201_0001_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Task attempt_local1574142201_0001_r_000000_0 is allowed to commit now
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local1574142201_0001_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step1/_temporary/0/task_local1574142201_0001_r_000000
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Task 'attempt_local1574142201_0001_r_000000_0' done.
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1574142201_0001_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=726
		FILE: Number of bytes written=317861
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=199
		HDFS: Number of bytes written=345
		HDFS: Number of read operations=14
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=5
		Reduce shuffle bytes=172
		Reduce input records=5
		Reduce output records=5
		Spilled Records=5
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=324534272
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=146
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1574142201_0001_r_000000_0
[INFO][2021-03-19 17:17|:46][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.Job]Job job_local1574142201_0001 completed successfully
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=1076
		FILE: Number of bytes written=635550
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=398
		HDFS: Number of bytes written=544
		HDFS: Number of read operations=25
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=21
		Map output records=21
		Map output bytes=220
		Map output materialized bytes=172
		Input split bytes=99
		Combine input records=21
		Combine output records=5
		Reduce input groups=5
		Reduce shuffle bytes=172
		Reduce input records=5
		Reduce output records=5
		Spilled Records=10
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=649068544
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=199
	File Output Format Counters 
		Bytes Written=146
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local282948423_0002
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.Job]Running job: job_local282948423_0002
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local282948423_0002_m_000000_0
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:47][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6be36f97
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+146
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 1116; bufvoid = 104857600
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214028(104856112); length = 369/6553600
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Task:attempt_local282948423_0002_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+146
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Task 'attempt_local282948423_0002_m_000000_0' done.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Final Counters for attempt_local282948423_0002_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=884
		FILE: Number of bytes written=633661
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=345
		HDFS: Number of bytes written=345
		HDFS: Number of read operations=24
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=5
		Map output records=93
		Map output bytes=1116
		Map output materialized bytes=608
		Input split bytes=106
		Combine input records=93
		Combine output records=43
		Spilled Records=43
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=371195904
	File Input Format Counters 
		Bytes Read=146
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local282948423_0002_m_000000_0
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local282948423_0002_r_000000_0
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@66d41cda
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6813fe70
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local282948423_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#2 about to shuffle output of map attempt_local282948423_0002_m_000000_0 decomp: 604 len: 608 to MEMORY
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 604 bytes from map-output for attempt_local282948423_0002_m_000000_0
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 604, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->604
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 594 bytes
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 604 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 608 bytes from disk
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 594 bytes
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Task:attempt_local282948423_0002_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Task attempt_local282948423_0002_r_000000_0 is allowed to commit now
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local282948423_0002_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step2/_temporary/0/task_local282948423_0002_r_000000
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Task 'attempt_local282948423_0002_r_000000_0' done.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.Task]Final Counters for attempt_local282948423_0002_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=2132
		FILE: Number of bytes written=634269
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=345
		HDFS: Number of bytes written=775
		HDFS: Number of read operations=27
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=12
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=43
		Reduce shuffle bytes=608
		Reduce input records=43
		Reduce output records=43
		Spilled Records=43
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=371195904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=430
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local282948423_0002_r_000000_0
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.Job]Job job_local282948423_0002 running in uber mode : false
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.Job]Job job_local282948423_0002 completed successfully
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=3016
		FILE: Number of bytes written=1267930
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=690
		HDFS: Number of bytes written=1120
		HDFS: Number of read operations=51
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=22
	Map-Reduce Framework
		Map input records=5
		Map output records=93
		Map output bytes=1116
		Map output materialized bytes=608
		Input split bytes=106
		Combine input records=93
		Combine output records=43
		Reduce input groups=43
		Reduce shuffle bytes=608
		Reduce input records=43
		Reduce output records=43
		Spilled Records=86
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=742391808
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=146
	File Output Format Counters 
		Bytes Written=430
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 17:17|:48][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local1046004051_0003
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.Job]Running job: job_local1046004051_0003
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1046004051_0003_m_000000_0
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@662d98e6
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+146
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 178; bufvoid = 104857600
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214316(104857264); length = 81/6553600
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Task:attempt_local1046004051_0003_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step1/part-00000:0+146
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Task 'attempt_local1046004051_0003_m_000000_0' done.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1046004051_0003_m_000000_0: Counters: 22
	File System Counters
		FILE: Number of bytes read=2290
		FILE: Number of bytes written=949821
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=491
		HDFS: Number of bytes written=775
		HDFS: Number of read operations=36
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Map-Reduce Framework
		Map input records=5
		Map output records=21
		Map output bytes=178
		Map output materialized bytes=226
		Input split bytes=106
		Combine input records=0
		Spilled Records=21
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=459276288
	File Input Format Counters 
		Bytes Read=146
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1046004051_0003_m_000000_0
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1046004051_0003_r_000000_0
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5fc43ea1
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5103cb29
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local1046004051_0003_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#3 about to shuffle output of map attempt_local1046004051_0003_m_000000_0 decomp: 222 len: 226 to MEMORY
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 222 bytes from map-output for attempt_local1046004051_0003_m_000000_0
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 222, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->222
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 216 bytes
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 222 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 226 bytes from disk
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 216 bytes
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Task:attempt_local1046004051_0003_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Task attempt_local1046004051_0003_r_000000_0 is allowed to commit now
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local1046004051_0003_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step3_1/_temporary/0/task_local1046004051_0003_r_000000
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Task 'attempt_local1046004051_0003_r_000000_0' done.
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1046004051_0003_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=2774
		FILE: Number of bytes written=950047
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=491
		HDFS: Number of bytes written=953
		HDFS: Number of read operations=39
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=226
		Reduce input records=21
		Reduce output records=21
		Spilled Records=21
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=459276288
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=178
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1046004051_0003_r_000000_0
[INFO][2021-03-19 17:17|:49][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.Job]Job job_local1046004051_0003 running in uber mode : false
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.Job]Job job_local1046004051_0003 completed successfully
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=5064
		FILE: Number of bytes written=1899868
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=982
		HDFS: Number of bytes written=1728
		HDFS: Number of read operations=75
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=34
	Map-Reduce Framework
		Map input records=5
		Map output records=21
		Map output bytes=178
		Map output materialized bytes=226
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=226
		Reduce input records=21
		Reduce output records=21
		Spilled Records=42
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=918552576
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=146
	File Output Format Counters 
		Bytes Written=178
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:1
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local1575109466_0004
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.Job]Running job: job_local1575109466_0004
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1575109466_0004_m_000000_0
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@214db664
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step2/part-00000:0+430
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 516; bufvoid = 104857600
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26214228(104856912); length = 169/6553600
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Task:attempt_local1575109466_0004_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step2/part-00000:0+430
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Task 'attempt_local1575109466_0004_m_000000_0' done.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1575109466_0004_m_000000_0: Counters: 22
	File System Counters
		FILE: Number of bytes read=2933
		FILE: Number of bytes written=1266518
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=921
		HDFS: Number of bytes written=953
		HDFS: Number of read operations=49
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=22
	Map-Reduce Framework
		Map input records=43
		Map output records=43
		Map output bytes=516
		Map output materialized bytes=608
		Input split bytes=106
		Combine input records=0
		Spilled Records=43
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=459276288
	File Input Format Counters 
		Bytes Read=430
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1575109466_0004_m_000000_0
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1575109466_0004_r_000000_0
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2f725a6d
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@21b5b1f3
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local1575109466_0004_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#4 about to shuffle output of map attempt_local1575109466_0004_m_000000_0 decomp: 604 len: 608 to MEMORY
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 604 bytes from map-output for attempt_local1575109466_0004_m_000000_0
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 604, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->604
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 594 bytes
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 1 segments, 604 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 608 bytes from disk
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 594 bytes
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Task:attempt_local1575109466_0004_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]1 / 1 copied.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Task attempt_local1575109466_0004_r_000000_0 is allowed to commit now
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local1575109466_0004_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step3_2/_temporary/0/task_local1575109466_0004_r_000000
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Task 'attempt_local1575109466_0004_r_000000_0' done.
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1575109466_0004_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=4181
		FILE: Number of bytes written=1267126
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=921
		HDFS: Number of bytes written=1383
		HDFS: Number of read operations=52
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=24
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=43
		Reduce shuffle bytes=608
		Reduce input records=43
		Reduce output records=43
		Spilled Records=43
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=459276288
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=430
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1575109466_0004_r_000000_0
[INFO][2021-03-19 17:17|:50][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.Job]Job job_local1575109466_0004 running in uber mode : false
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.Job]Job job_local1575109466_0004 completed successfully
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=7114
		FILE: Number of bytes written=2533644
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1842
		HDFS: Number of bytes written=2336
		HDFS: Number of read operations=101
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=46
	Map-Reduce Framework
		Map input records=43
		Map output records=43
		Map output bytes=516
		Map output materialized bytes=608
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=43
		Reduce shuffle bytes=608
		Reduce input records=43
		Reduce output records=43
		Spilled Records=86
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=918552576
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=430
	File Output Format Counters 
		Bytes Written=430
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.conf.Configuration.deprecation]mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.metrics.jvm.JvmMetrics]Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
[WARN][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.JobResourceUploader]Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
[WARN][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.JobResourceUploader]No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 2
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.JobSubmitter]number of splits:2
[INFO][2021-03-19 17:17|:51][org.apache.hadoop.mapreduce.JobSubmitter]Submitting tokens for job: job_local1484639334_0005
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.Job]The url to track the job: http://localhost:8080/
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.Job]Running job: job_local1484639334_0005
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter set in config null
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Waiting for map tasks
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1484639334_0005_m_000000_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@276beea9
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step3_2/part-00000:0+430
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task:attempt_local1484639334_0005_m_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step3_2/part-00000:0+430
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task 'attempt_local1484639334_0005_m_000000_0' done.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1484639334_0005_m_000000_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=4461
		FILE: Number of bytes written=1584967
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1351
		HDFS: Number of bytes written=1383
		HDFS: Number of read operations=64
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=28
	Map-Reduce Framework
		Map input records=43
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=108
		Combine input records=0
		Combine output records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=459800576
	File Input Format Counters 
		Bytes Read=430
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1484639334_0005_m_000000_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1484639334_0005_m_000001_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4ead872b
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Processing split: hdfs://192.168.195.11:9000/data/input/step3_1/part-00000:0+178
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]numReduceTasks: 1
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask](EQUATOR) 0 kvi 26214396(104857584)
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]mapreduce.task.io.sort.mb: 100
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]soft limit at 83886080
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufvoid = 104857600
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]kvstart = 26214396; length = 6553600
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Starting flush of map output
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Spilling map output
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]bufstart = 0; bufend = 1656; bufvoid = 104857600
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]kvstart = 26214396(104857584); kvend = 26213860(104855440); length = 537/6553600
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.MapTask]Finished spill 0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task:attempt_local1484639334_0005_m_000001_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]hdfs://192.168.195.11:9000/data/input/step3_1/part-00000:0+178
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task 'attempt_local1484639334_0005_m_000001_0' done.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1484639334_0005_m_000001_0: Counters: 23
	File System Counters
		FILE: Number of bytes read=4696
		FILE: Number of bytes written=1585527
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1529
		HDFS: Number of bytes written=1383
		HDFS: Number of read operations=66
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=28
	Map-Reduce Framework
		Map input records=21
		Map output records=135
		Map output bytes=1656
		Map output materialized bytes=528
		Input split bytes=108
		Combine input records=135
		Combine output records=35
		Spilled Records=35
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=459800576
	File Input Format Counters 
		Bytes Read=178
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1484639334_0005_m_000001_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]map task executor complete.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Waiting for reduce tasks
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Starting task: attempt_local1484639334_0005_r_000000_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]File Output Committer Algorithm version is 1
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.yarn.util.ProcfsBasedProcessTree]ProcfsBasedProcessTree currently is supported only on Linux.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task] Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@33e2e451
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.ReduceTask]Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3d251c5d
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]MergerManager: memoryLimit=2657091584, maxSingleShuffleLimit=664272896, mergeThreshold=1753680512, ioSortFactor=10, memToMemMergeOutputsThreshold=10
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]attempt_local1484639334_0005_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#5 about to shuffle output of map attempt_local1484639334_0005_m_000000_0 decomp: 2 len: 6 to MEMORY
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 2 bytes from map-output for attempt_local1484639334_0005_m_000000_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.LocalFetcher]localfetcher#5 about to shuffle output of map attempt_local1484639334_0005_m_000001_0 decomp: 524 len: 528 to MEMORY
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput]Read 524 bytes from map-output for attempt_local1484639334_0005_m_000001_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]closeInMemoryFile -> map-output of size: 524, inMemoryMapOutputs.size() -> 2, commitMemory -> 2, usedMemory ->526
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.EventFetcher]EventFetcher is interrupted.. Returning
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]2 / 2 copied.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Merger]Merging 2 sorted segments
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 518 bytes
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merged 2 segments, 526 bytes to disk to satisfy reduce memory limit
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 1 files, 528 bytes from disk
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl]Merging 0 segments, 0 bytes from memory into reduce
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Merger]Merging 1 sorted segments
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Merger]Down to the last merge-pass, with 1 segments left of total size: 518 bytes
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]2 / 2 copied.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task:attempt_local1484639334_0005_r_000000_0 is done. And is in the process of committing
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]2 / 2 copied.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task attempt_local1484639334_0005_r_000000_0 is allowed to commit now
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter]Saved output of task 'attempt_local1484639334_0005_r_000000_0' to hdfs://192.168.195.11:9000/data/input/step4/_temporary/0/task_local1484639334_0005_r_000000
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]reduce > reduce
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Task 'attempt_local1484639334_0005_r_000000_0' done.
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.Task]Final Counters for attempt_local1484639334_0005_r_000000_0: Counters: 29
	File System Counters
		FILE: Number of bytes read=5822
		FILE: Number of bytes written=1586055
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1529
		HDFS: Number of bytes written=1765
		HDFS: Number of read operations=69
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Map-Reduce Framework
		Combine input records=0
		Combine output records=0
		Reduce input groups=5
		Reduce shuffle bytes=534
		Reduce input records=35
		Reduce output records=35
		Spilled Records=35
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=459800576
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Output Format Counters 
		Bytes Written=382
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]Finishing task: attempt_local1484639334_0005_r_000000_0
[INFO][2021-03-19 17:17|:52][org.apache.hadoop.mapred.LocalJobRunner]reduce task executor complete.
[INFO][2021-03-19 17:17|:53][org.apache.hadoop.mapreduce.Job]Job job_local1484639334_0005 running in uber mode : false
[INFO][2021-03-19 17:17|:53][org.apache.hadoop.mapreduce.Job] map 100% reduce 100%
[INFO][2021-03-19 17:17|:53][org.apache.hadoop.mapreduce.Job]Job job_local1484639334_0005 completed successfully
[INFO][2021-03-19 17:17|:53][org.apache.hadoop.mapreduce.Job]Counters: 35
	File System Counters
		FILE: Number of bytes read=14979
		FILE: Number of bytes written=4756549
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=4409
		HDFS: Number of bytes written=4531
		HDFS: Number of read operations=199
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=86
	Map-Reduce Framework
		Map input records=64
		Map output records=135
		Map output bytes=1656
		Map output materialized bytes=534
		Input split bytes=216
		Combine input records=135
		Combine output records=35
		Reduce input groups=5
		Reduce shuffle bytes=534
		Reduce input records=35
		Reduce output records=35
		Spilled Records=70
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=1379401728
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=608
	File Output Format Counters 
		Bytes Written=382
[INFO][2021-04-09 21:57|:15][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-04-09 21:57|:16][org.apache.spark.SparkContext]Submitted application: KMeansExample
[INFO][2021-04-09 21:57|:16][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-04-09 21:57|:16][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-04-09 21:57|:16][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-04-09 21:57|:16][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-04-09 21:57|:16][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-04-09 21:57|:17][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 4266.
[INFO][2021-04-09 21:57|:17][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-04-09 21:57|:17][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-04-09 21:57|:17][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-04-09 21:57|:17][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-04-09 21:57|:17][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-00946e09-7a4e-4151-b1bd-fab514c4baba
[INFO][2021-04-09 21:57|:17][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-04-09 21:57|:17][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-04-09 21:57|:18][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-04-09 21:57|:18][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-04-09 21:57|:18][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2021-04-09 21:57|:18][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4279.
[INFO][2021-04-09 21:57|:18][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:4279
[INFO][2021-04-09 21:57|:18][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-04-09 21:57|:18][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 4279, None)
[INFO][2021-04-09 21:57|:18][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:4279 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 4279, None)
[INFO][2021-04-09 21:57|:18][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 4279, None)
[INFO][2021-04-09 21:57|:18][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 4279, None)
[INFO][2021-04-09 21:57|:19][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 250.7 KB, free 1991.8 MB)
[INFO][2021-04-09 21:57|:19][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 1991.7 MB)
[INFO][2021-04-09 21:57|:19][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.1:4279 (size: 23.9 KB, free: 1992.0 MB)
[INFO][2021-04-09 21:57|:19][org.apache.spark.SparkContext]Created broadcast 0 from main at <unknown>:0
[INFO][2021-04-09 21:57|:40][org.apache.hadoop.ipc.Client]Retrying connect to server: spark1/192.168.195.11:9000. Already tried 0 time(s); maxRetries=45
[INFO][2021-04-09 21:58|:00][org.apache.hadoop.ipc.Client]Retrying connect to server: spark1/192.168.195.11:9000. Already tried 1 time(s); maxRetries=45
[INFO][2021-04-09 21:58|:20][org.apache.hadoop.ipc.Client]Retrying connect to server: spark1/192.168.195.11:9000. Already tried 2 time(s); maxRetries=45
[INFO][2021-04-10 22:03|:27][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-04-10 22:03|:28][org.apache.spark.SparkContext]Submitted application: kmeans
[INFO][2021-04-10 22:03|:29][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-04-10 22:03|:29][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-04-10 22:03|:29][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-04-10 22:03|:29][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-04-10 22:03|:29][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-04-10 22:03|:30][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 4443.
[INFO][2021-04-10 22:03|:30][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-04-10 22:03|:30][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-04-10 22:03|:30][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-04-10 22:03|:30][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-04-10 22:03|:30][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-10e7238d-c0ee-4617-9171-fe53ca4c1e2d
[INFO][2021-04-10 22:03|:30][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-04-10 22:03|:30][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-04-10 22:03|:30][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-04-10 22:03|:31][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-04-10 22:03|:31][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2021-04-10 22:03|:31][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 4466.
[INFO][2021-04-10 22:03|:31][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:4466
[INFO][2021-04-10 22:03|:31][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-04-10 22:03|:31][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 4466, None)
[INFO][2021-04-10 22:03|:31][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:4466 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 4466, None)
[INFO][2021-04-10 22:03|:31][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 4466, None)
[INFO][2021-04-10 22:03|:31][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 4466, None)
[INFO][2021-04-10 22:03|:32][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 250.7 KB, free 1991.8 MB)
[INFO][2021-04-10 22:03|:32][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:32][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.1:4466 (size: 23.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:32][org.apache.spark.SparkContext]Created broadcast 0 from main at <unknown>:0
[INFO][2021-04-10 22:03|:33][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-04-10 22:03|:34][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Registering RDD 2 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Got job 0 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:34][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 983 bytes result sent to driver
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 1026 bytes result sent to driver
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 578 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 560 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 0 (main at <unknown>:0) finished in 0.608 s
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 1)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[5] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 1095 bytes result sent to driver
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 1138 bytes result sent to driver
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 96 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 96 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (main at <unknown>:0) finished in 0.098 s
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: main at <unknown>:0, took 0.887873 s
[INFO][2021-04-10 22:03|:34][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Registering RDD 6 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Got job 1 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[6] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:34][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[6] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:34][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2021-04-10 22:03|:34][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2021-04-10 22:03|:34][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 68 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 93 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (main at <unknown>:0) finished in 0.094 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[9] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 4 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 21 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1009 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 33 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (main at <unknown>:0) finished in 0.035 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: main at <unknown>:0, took 0.164399 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Registering RDD 10 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Got job 2 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 4)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4 (MapPartitionsRDD[10] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[10] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 8)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 4.0 (TID 9)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 8). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 8) in 58 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 4.0 (TID 9). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 4.0 (TID 9) in 94 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 4 (main at <unknown>:0) finished in 0.105 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[13] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 5.0 (TID 11, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 5.0 (TID 11)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 10)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 5.0 (TID 11). 1009 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 5.0 (TID 11) in 15 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 10). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 10) in 21 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (main at <unknown>:0) finished in 0.022 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: main at <unknown>:0, took 0.168252 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Registering RDD 14 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Got job 3 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 12, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 6.0 (TID 13, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 12)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 6.0 (TID 13)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 6.0 (TID 13). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 6.0 (TID 13) in 54 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 12). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 12) in 85 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (main at <unknown>:0) finished in 0.086 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[17] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[17] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 14, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 7.0 (TID 15, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 14)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 7.0 (TID 15)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 7.0 (TID 15). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 7.0 (TID 15) in 18 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 14). 1009 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 14) in 29 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (main at <unknown>:0) finished in 0.030 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: main at <unknown>:0, took 0.185878 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Got job 4 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[18] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[18] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 16, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 8.0 (TID 17, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 16)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 8.0 (TID 17)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 16). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 16) in 57 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 8.0 (TID 17). 983 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 8.0 (TID 17) in 85 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.195.1:4466 in memory (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (main at <unknown>:0) finished in 0.096 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[21] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[21] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.ContextCleaner]Cleaned shuffle 1
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 18, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 9.0 (TID 19, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 18)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 9.0 (TID 19)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.195.1:4466 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 18). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 9.0 (TID 19). 1009 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 18) in 26 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 9.0 (TID 19) in 26 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (main at <unknown>:0) finished in 0.033 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.195.1:4466 in memory (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: main at <unknown>:0, took 0.174802 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Registering RDD 22 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Got job 5 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 10)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.195.1:4466 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.ContextCleaner]Cleaned shuffle 2
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.195.1:4466 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.195.1:4466 in memory (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.ContextCleaner]Cleaned shuffle 3
[INFO][2021-04-10 22:03|:35][org.apache.spark.ContextCleaner]Cleaned shuffle 0
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.195.1:4466 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 20, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 10.0 (TID 21, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 20)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 10.0 (TID 21)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 10.0 (TID 21). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 10.0 (TID 21) in 32 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 20). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 20) in 53 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 10 (main at <unknown>:0) finished in 0.047 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 11)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[25] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.195.1:4466 in memory (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[25] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 22, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 11.0 (TID 23, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 11.0 (TID 23)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 22)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 1.0 in stage 11.0 (TID 23). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 22). 1095 bytes result sent to driver
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 11.0 (TID 23) in 25 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 22) in 26 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (main at <unknown>:0) finished in 0.029 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: main at <unknown>:0, took 0.157082 s
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Registering RDD 26 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Got job 6 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 12)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 12)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 12 (MapPartitionsRDD[26] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:35][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[26] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 2 tasks
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 24, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 12.0 (TID 25, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 24)
[INFO][2021-04-10 22:03|:35][org.apache.spark.executor.Executor]Running task 1.0 in stage 12.0 (TID 25)
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:35][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 1.0 in stage 12.0 (TID 25). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 24). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 12.0 (TID 25) in 34 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 24) in 37 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 12 (main at <unknown>:0) finished in 0.038 s
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 13)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[29] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[29] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 2 tasks
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 26, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 13.0 (TID 27, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Running task 1.0 in stage 13.0 (TID 27)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 26)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 1.0 in stage 13.0 (TID 27). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 26). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 13.0 (TID 27) in 12 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 26) in 13 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (main at <unknown>:0) finished in 0.002 s
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: main at <unknown>:0, took 0.082832 s
[INFO][2021-04-10 22:03|:36][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Registering RDD 30 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Got job 7 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (main at <unknown>:0)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 14)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 14)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 14 (MapPartitionsRDD[30] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.195.1:4466 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[30] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 2 tasks
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 28, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 14.0 (TID 29, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 28)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Running task 1.0 in stage 14.0 (TID 29)
[INFO][2021-04-10 22:03|:36][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-10 22:03|:36][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 28). 940 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 28) in 54 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 1.0 in stage 14.0 (TID 29). 983 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 14.0 (TID 29) in 56 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 14 (main at <unknown>:0) finished in 0.059 s
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 15)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[33] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.195.1:4466 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-10 22:03|:36][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[33] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 2 tasks
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 30, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 15.0 (TID 31, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 30)
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Running task 1.0 in stage 15.0 (TID 31)
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 30). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.executor.Executor]Finished task 1.0 in stage 15.0 (TID 31). 1052 bytes result sent to driver
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 30) in 10 ms on localhost (executor driver) (1/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 15.0 (TID 31) in 10 ms on localhost (executor driver) (2/2)
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (main at <unknown>:0) finished in 0.011 s
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: main at <unknown>:0, took 0.096834 s
[INFO][2021-04-10 22:03|:36][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2021-04-10 22:03|:36][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.195.1:4040
[INFO][2021-04-10 22:03|:36][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2021-04-10 22:03|:36][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2021-04-10 22:03|:36][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2021-04-10 22:03|:36][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2021-04-10 22:03|:36][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2021-04-10 22:03|:36][org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-59a385b7-08f0-4adb-9caf-61e908593cd9
[INFO][2021-04-13 10:47|:17][org.apache.spark.SparkContext]Running Spark version 2.2.2
[INFO][2021-04-13 10:47|:18][org.apache.spark.SparkContext]Submitted application: kmeans
[INFO][2021-04-13 10:47|:18][org.apache.spark.SecurityManager]Changing view acls to: Administrator
[INFO][2021-04-13 10:47|:18][org.apache.spark.SecurityManager]Changing modify acls to: Administrator
[INFO][2021-04-13 10:47|:18][org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO][2021-04-13 10:47|:18][org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO][2021-04-13 10:47|:18][org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
[INFO][2021-04-13 10:47|:20][org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 9717.
[INFO][2021-04-13 10:47|:20][org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO][2021-04-13 10:47|:20][org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManagerMasterEndpoint]Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManagerMasterEndpoint]BlockManagerMasterEndpoint up
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-2065c8fe-de65-45ca-b63b-e31cc9e2c34d
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 1992.0 MB
[INFO][2021-04-13 10:47|:20][org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO][2021-04-13 10:47|:20][org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO][2021-04-13 10:47|:20][org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://192.168.195.1:4040
[INFO][2021-04-13 10:47|:20][org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO][2021-04-13 10:47|:20][org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 9738.
[INFO][2021-04-13 10:47|:20][org.apache.spark.network.netty.NettyBlockTransferService]Server created on 192.168.195.1:9738
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManager]Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 192.168.195.1, 9738, None)
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 192.168.195.1:9738 with 1992.0 MB RAM, BlockManagerId(driver, 192.168.195.1, 9738, None)
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 192.168.195.1, 9738, None)
[INFO][2021-04-13 10:47|:20][org.apache.spark.storage.BlockManager]Initialized BlockManager: BlockManagerId(driver, 192.168.195.1, 9738, None)
[INFO][2021-04-13 10:47|:22][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 250.7 KB, free 1991.8 MB)
[INFO][2021-04-13 10:47|:22][org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.9 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:22][org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 192.168.195.1:9738 (size: 23.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:22][org.apache.spark.SparkContext]Created broadcast 0 from main at <unknown>:0
[INFO][2021-04-13 10:47|:23][org.apache.hadoop.mapred.FileInputFormat]Total input paths to process : 1
[INFO][2021-04-13 10:47|:23][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Registering RDD 2 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Got job 0 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 0)
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 0)
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:23][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:23][org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:23][org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:23][org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 2 tasks
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:23][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:23][org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO][2021-04-13 10:47|:23][org.apache.spark.executor.Executor]Running task 1.0 in stage 0.0 (TID 1)
[INFO][2021-04-13 10:47|:24][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:24][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 1.0 in stage 0.0 (TID 1). 1069 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 0.0 in stage 0.0 (TID 0). 1069 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 0.0 (TID 0) in 628 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 0.0 (TID 1) in 611 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 0 (main at <unknown>:0) finished in 0.662 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 1)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (MapPartitionsRDD[5] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 2 tasks
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 1.0 in stage 1.0 (TID 3)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 1.0 in stage 1.0 (TID 3). 1095 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 2). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 1.0 (TID 3) in 94 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 2) in 96 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (main at <unknown>:0) finished in 0.098 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Job 0 finished: main at <unknown>:0, took 0.966745 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Registering RDD 6 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Got job 1 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 2 (MapPartitionsRDD[6] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[6] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 2 tasks
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 4)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 1.0 in stage 2.0 (TID 5)
[INFO][2021-04-13 10:47|:24][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:24][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 4). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 4) in 55 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 1.0 in stage 2.0 (TID 5). 983 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 2.0 (TID 5) in 62 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 2 (main at <unknown>:0) finished in 0.063 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 3)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (MapPartitionsRDD[9] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 2 tasks
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 3.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 6)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 1.0 in stage 3.0 (TID 7)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 6). 1009 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 1.0 in stage 3.0 (TID 7). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 6) in 19 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 3.0 (TID 7) in 20 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (main at <unknown>:0) finished in 0.023 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Job 1 finished: main at <unknown>:0, took 0.119433 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Registering RDD 10 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Got job 2 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 4)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 4 (MapPartitionsRDD[10] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:24][org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[10] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 2 tasks
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 8, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 4.0 (TID 9, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 8)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Running task 1.0 in stage 4.0 (TID 9)
[INFO][2021-04-13 10:47|:24][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:24][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 8). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 8) in 91 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.executor.Executor]Finished task 1.0 in stage 4.0 (TID 9). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 4.0 (TID 9) in 101 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 4 (main at <unknown>:0) finished in 0.102 s
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 5)
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:24][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (MapPartitionsRDD[13] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:24][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 10, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 5.0 (TID 11, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 5.0 (TID 11)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 10)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 10). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 5.0 (TID 11). 1009 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 10) in 54 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 5.0 (TID 11) in 54 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (main at <unknown>:0) finished in 0.060 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Job 2 finished: main at <unknown>:0, took 0.203508 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Registering RDD 14 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Got job 3 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 6)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 12, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 6.0 (TID 13, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 12)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 6.0 (TID 13)
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 6.0 (TID 13). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 6.0 (TID 13) in 176 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 12). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 12) in 181 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 6 (main at <unknown>:0) finished in 0.179 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 7)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (MapPartitionsRDD[17] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 6.7 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[17] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 14, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 7.0 (TID 15, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 14)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 7.0 (TID 15)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 14). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 14) in 23 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 7.0 (TID 15). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 7.0 (TID 15) in 25 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (main at <unknown>:0) finished in 0.027 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Job 3 finished: main at <unknown>:0, took 0.262843 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Got job 4 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 8)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 8 (MapPartitionsRDD[18] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 5.3 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.7 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[18] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 16, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 8.0 (TID 17, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 16)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 8.0 (TID 17)
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 8.0 (TID 17). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 8.0 (TID 17) in 38 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 16). 983 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 16) in 49 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 8 (main at <unknown>:0) finished in 0.050 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 9)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (MapPartitionsRDD[21] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 6.7 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[21] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 18, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 9.0 (TID 19, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 18)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 9.0 (TID 19)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 9.0 (TID 19). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 18). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 9.0 (TID 19) in 15 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 18) in 17 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (main at <unknown>:0) finished in 0.019 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Job 4 finished: main at <unknown>:0, took 0.088498 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Registering RDD 22 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Got job 5 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 10)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 5.3 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 20, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 10.0 (TID 21, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 20)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 10.0 (TID 21)
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 10.0 (TID 21). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 10.0 (TID 21) in 40 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 20). 983 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 20) in 47 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 10 (main at <unknown>:0) finished in 0.048 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 11)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (MapPartitionsRDD[25] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 6.7 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[25] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 22, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 11.0 (TID 23, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 11.0 (TID 23)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 22)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 11.0 (TID 23). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 11.0 (TID 23) in 12 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 22). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 22) in 25 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (main at <unknown>:0) finished in 0.026 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Job 5 finished: main at <unknown>:0, took 0.101612 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Registering RDD 26 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Got job 6 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 12)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 12)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 12 (MapPartitionsRDD[26] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 5.3 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[26] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 24, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 12.0 (TID 25, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 24)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 12.0 (TID 25)
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 24). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 24) in 28 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 12.0 (TID 25). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 12.0 (TID 25) in 32 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 12 (main at <unknown>:0) finished in 0.033 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 13)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (MapPartitionsRDD[29] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 6.7 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 13 (MapPartitionsRDD[29] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 26, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 13.0 (TID 27, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 26)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 13.0 (TID 27)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 13.0 (TID 27). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 26). 1009 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 13.0 (TID 27) in 16 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 26) in 17 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (main at <unknown>:0) finished in 0.020 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Job 6 finished: main at <unknown>:0, took 0.072055 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Starting job: main at <unknown>:0
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Registering RDD 30 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Got job 7 (main at <unknown>:0) with 2 output partitions
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (main at <unknown>:0)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 14)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Missing parents: List(ShuffleMapStage 14)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ShuffleMapStage 14 (MapPartitionsRDD[30] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 5.3 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.9 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 192.168.195.1:9738 (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[30] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 28, localhost, executor driver, partition 0, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 14.0 (TID 29, localhost, executor driver, partition 1, ANY, 4851 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 28)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 14.0 (TID 29)
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:0+1753
[INFO][2021-04-13 10:47|:25][org.apache.spark.rdd.HadoopRDD]Input split: hdfs://192.168.195.11:9000/data/k-means.dat:1753+1754
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 28). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 28) in 43 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 3
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 1
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 14.0 (TID 29). 940 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 14.0 (TID 29) in 74 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ShuffleMapStage 14 (main at <unknown>:0) finished in 0.076 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]looking for newly runnable stages
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]running: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]waiting: Set(ResultStage 15)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]failed: Set()
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (MapPartitionsRDD[33] at main at <unknown>:0), which has no missing parents
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 6.7 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 4
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 3.4 KB, free 1991.6 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 192.168.195.1:9738 (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1015
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Submitting 2 missing tasks from ResultStage 15 (MapPartitionsRDD[33] at main at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1))
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 2 tasks
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 30, localhost, executor driver, partition 0, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Starting task 1.0 in stage 15.0 (TID 31, localhost, executor driver, partition 1, ANY, 4621 bytes)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 30)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Running task 1.0 in stage 15.0 (TID 31)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 2 non-empty blocks out of 2 blocks
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 2
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 1.0 in stage 15.0 (TID 31). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 30). 1052 bytes result sent to driver
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 0
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 1.0 in stage 15.0 (TID 31) in 11 ms on localhost (executor driver) (1/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 30) in 13 ms on localhost (executor driver) (2/2)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (main at <unknown>:0) finished in 0.015 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.scheduler.DAGScheduler]Job 7 finished: main at <unknown>:0, took 0.134442 s
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1991.9 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 5
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 192.168.195.1:9738 in memory (size: 3.4 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://192.168.195.1:4040
[INFO][2021-04-13 10:47|:25][org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 192.168.195.1:9738 in memory (size: 2.9 KB, free: 1992.0 MB)
[INFO][2021-04-13 10:47|:25][org.apache.spark.ContextCleaner]Cleaned shuffle 6
[INFO][2021-04-13 10:47|:25][org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO][2021-04-13 10:47|:26][org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO][2021-04-13 10:47|:26][org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO][2021-04-13 10:47|:26][org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO][2021-04-13 10:47|:26][org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO][2021-04-13 10:47|:26][org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO][2021-04-13 10:47|:26][org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO][2021-04-13 10:47|:26][org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-dd12a0aa-cb3a-41dd-9a37-dfd3237a2f19
